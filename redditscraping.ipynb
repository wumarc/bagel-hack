{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape Reddit posts...\n",
      "Scraping posts from r/jobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_21236\\2635829203.py:55: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cutoff_date = datetime.utcnow() - timedelta(days=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found networking-related post 1/1000\n",
      "Found networking-related post 2/1000\n",
      "Found networking-related post 3/1000\n",
      "Found networking-related post 4/1000\n",
      "Found networking-related post 5/1000\n",
      "Found networking-related post 6/1000\n",
      "Found networking-related post 7/1000\n",
      "Found networking-related post 8/1000\n",
      "Found networking-related post 9/1000\n",
      "Found networking-related post 10/1000\n",
      "Found networking-related post 11/1000\n",
      "Found networking-related post 12/1000\n",
      "Found networking-related post 13/1000\n",
      "Found networking-related post 14/1000\n",
      "Found networking-related post 15/1000\n",
      "Found networking-related post 16/1000\n",
      "Found networking-related post 17/1000\n",
      "Found networking-related post 18/1000\n",
      "Found networking-related post 19/1000\n",
      "Found networking-related post 20/1000\n",
      "Found networking-related post 21/1000\n",
      "Found networking-related post 22/1000\n",
      "Found networking-related post 23/1000\n",
      "Found networking-related post 24/1000\n",
      "Found networking-related post 25/1000\n",
      "Found networking-related post 26/1000\n",
      "Found networking-related post 27/1000\n",
      "Found networking-related post 28/1000\n",
      "Found networking-related post 29/1000\n",
      "Found networking-related post 30/1000\n",
      "Found networking-related post 31/1000\n",
      "Found networking-related post 32/1000\n",
      "Found networking-related post 33/1000\n",
      "Found networking-related post 34/1000\n",
      "Found networking-related post 35/1000\n",
      "Found networking-related post 36/1000\n",
      "Found networking-related post 37/1000\n",
      "Found networking-related post 38/1000\n",
      "Found networking-related post 39/1000\n",
      "Found networking-related post 40/1000\n",
      "Found networking-related post 41/1000\n",
      "Found networking-related post 42/1000\n",
      "Found networking-related post 43/1000\n",
      "Found networking-related post 44/1000\n",
      "Found networking-related post 45/1000\n",
      "Found networking-related post 46/1000\n",
      "Found networking-related post 47/1000\n",
      "Found networking-related post 48/1000\n",
      "Found networking-related post 49/1000\n",
      "Found networking-related post 50/1000\n",
      "Found networking-related post 51/1000\n",
      "Found networking-related post 52/1000\n",
      "Found networking-related post 53/1000\n",
      "Found networking-related post 54/1000\n",
      "Found networking-related post 55/1000\n",
      "Found networking-related post 56/1000\n",
      "Found networking-related post 57/1000\n",
      "Found networking-related post 58/1000\n",
      "Found networking-related post 59/1000\n",
      "Found networking-related post 60/1000\n",
      "Found networking-related post 61/1000\n",
      "Found networking-related post 62/1000\n",
      "Found networking-related post 63/1000\n",
      "Found networking-related post 64/1000\n",
      "Found networking-related post 65/1000\n",
      "Found networking-related post 66/1000\n",
      "Found networking-related post 67/1000\n",
      "Found networking-related post 68/1000\n",
      "Found networking-related post 69/1000\n",
      "Found networking-related post 70/1000\n",
      "Found networking-related post 71/1000\n",
      "Found networking-related post 72/1000\n",
      "Found networking-related post 73/1000\n",
      "Found networking-related post 74/1000\n",
      "Found networking-related post 75/1000\n",
      "Found networking-related post 76/1000\n",
      "Found networking-related post 77/1000\n",
      "Found networking-related post 78/1000\n",
      "Found networking-related post 79/1000\n",
      "Found networking-related post 80/1000\n",
      "Found networking-related post 81/1000\n",
      "Found networking-related post 82/1000\n",
      "Found networking-related post 83/1000\n",
      "Found networking-related post 84/1000\n",
      "Found networking-related post 85/1000\n",
      "Found networking-related post 86/1000\n",
      "Found networking-related post 87/1000\n",
      "Found networking-related post 88/1000\n",
      "Found networking-related post 89/1000\n",
      "Found networking-related post 90/1000\n",
      "Found networking-related post 91/1000\n",
      "Found networking-related post 92/1000\n",
      "Total networking-related posts found: 92\n",
      "\n",
      "DataFrame columns: ['title', 'text', 'score', 'created_utc', 'num_comments']\n",
      "\n",
      "Analyzing sentiment...\n",
      "Average sentiment: 0.107\n",
      "\n",
      "Analyzing common problems...\n",
      "\n",
      "Top 5 Most Common Networking Problems:\n",
      "1. feeling stuck after months of job search-  9 years of work exp- need advice & support (mentioned 1 times)\n",
      "2. hello all, \n",
      "\n",
      "feeling stuck after months of job searching – need advice & support\n",
      "\n",
      "after months of job hunting on linkedin, indeed, and naukri, i’m reaching out because i’m almost out of my safety net, my rsus and savings are nearly gone (mentioned 1 times)\n",
      "3. having started as an associate myself, i’ve learned through every challenge along the way (mentioned 1 times)\n",
      "4. legal hurdles and personal challenges forced me to return to india, and now i’m struggling to get back on my feet (mentioned 1 times)\n",
      "5. i have a work permit, so visa sponsorship isn’t an issue, but being in the uk has made the search challenging (mentioned 1 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_21236\\2635829203.py:145: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results...\n",
      "Analysis complete! Results saved to:\n",
      "- networking_posts.csv\n",
      "- networking_problems.txt\n",
      "- sentiment_distribution.png\n",
      "- top_problems.png\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"nNt1apRLcond1DKKcfhMJg\",\n",
    "    client_secret=\"OYprE9hUaZriB4lRixxH6iq3duoZXw\",\n",
    "    user_agent=\"script:networking_analysis:v1.0 (by /u/mirk01)\"\n",
    ")\n",
    "\n",
    "# Keywords related to networking issues\n",
    "networking_keywords = [\n",
    "    'networking', 'network', 'connections', 'connect', 'referral', 'referrals',\n",
    "    'linkedin', 'linkedin profile', 'professional network', 'reach out',\n",
    "    'cold email', 'cold message', 'cold call', 'informational interview'\n",
    "]\n",
    "\n",
    "def is_networking_related(text):\n",
    "    text = text.lower()\n",
    "    return any(keyword in text for keyword in networking_keywords)\n",
    "\n",
    "def extract_problems(text):\n",
    "    # Common problem indicators\n",
    "    problem_indicators = [\n",
    "        'difficult', 'hard', 'trouble', 'struggle', 'problem', 'issue',\n",
    "        'challenge', 'frustrated', 'anxious', 'nervous', 'afraid', 'scared',\n",
    "        'don\\'t know', 'confused', 'overwhelmed', 'stuck'\n",
    "    ]\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split('[.!?]+', text.lower())\n",
    "    \n",
    "    # Extract sentences containing problem indicators\n",
    "    problems = []\n",
    "    for sentence in sentences:\n",
    "        if any(indicator in sentence for indicator in problem_indicators):\n",
    "            problems.append(sentence.strip())\n",
    "    \n",
    "    return problems\n",
    "\n",
    "def scrape_reddit_posts(num_posts=1000):\n",
    "    posts = []\n",
    "    problems = []\n",
    "    subreddit = reddit.subreddit('jobs')\n",
    "    \n",
    "    print(f\"Scraping posts from r/jobs...\")\n",
    "    \n",
    "    # Get posts from the last 30 days\n",
    "    cutoff_date = datetime.utcnow() - timedelta(days=30)\n",
    "    \n",
    "    for post in subreddit.hot(limit=num_posts):\n",
    "        # Check if post is from within our time window\n",
    "        post_date = datetime.fromtimestamp(post.created_utc)\n",
    "        if post_date < cutoff_date:\n",
    "            continue\n",
    "            \n",
    "        # Check title and selftext\n",
    "        if is_networking_related(post.title) or is_networking_related(post.selftext):\n",
    "            posts.append({\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'score': post.score,\n",
    "                'created_utc': post_date,\n",
    "                'num_comments': post.num_comments\n",
    "            })\n",
    "            \n",
    "            # Extract problems\n",
    "            problems.extend(extract_problems(post.title))\n",
    "            problems.extend(extract_problems(post.selftext))\n",
    "            \n",
    "            print(f\"Found networking-related post {len(posts)}/{num_posts}\")\n",
    "            \n",
    "            if len(posts) >= num_posts:\n",
    "                break\n",
    "    \n",
    "    return posts, problems\n",
    "\n",
    "def main():\n",
    "    # Install required packages if not already installed\n",
    "    try:\n",
    "        import praw\n",
    "        import pandas as pd\n",
    "        from textblob import TextBlob\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "    except ImportError:\n",
    "        print(\"Installing required packages...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call(['pip', 'install', 'praw', 'pandas', 'textblob', 'matplotlib', 'seaborn'])\n",
    "        print(\"Packages installed successfully!\")\n",
    "\n",
    "    # Scrape posts\n",
    "    print(\"Starting to scrape Reddit posts...\")\n",
    "    posts, problems = scrape_reddit_posts(1000)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(posts)\n",
    "    print(f\"Total networking-related posts found: {len(df)}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"No networking-related posts were found. Please check the scraping process.\")\n",
    "        return\n",
    "    \n",
    "    # Print DataFrame columns for debugging\n",
    "    print(\"\\nDataFrame columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Analyze sentiment\n",
    "    print(\"\\nAnalyzing sentiment...\")\n",
    "    df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "    # Plot sentiment distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x='sentiment', bins=30)\n",
    "    plt.title('Sentiment Distribution of Networking-Related Posts')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('sentiment_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Average sentiment: {df['sentiment'].mean():.3f}\")\n",
    "    \n",
    "    # Analyze common problems\n",
    "    print(\"\\nAnalyzing common problems...\")\n",
    "    problem_counter = Counter(problems)\n",
    "    top_5_problems = problem_counter.most_common(5)\n",
    "    \n",
    "    print(\"\\nTop 5 Most Common Networking Problems:\")\n",
    "    for i, (problem, count) in enumerate(top_5_problems, 1):\n",
    "        print(f\"{i}. {problem} (mentioned {count} times)\")\n",
    "    \n",
    "    # Plot top 5 problems\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    problems, counts = zip(*top_5_problems)\n",
    "    plt.bar(range(len(problems)), counts)\n",
    "    plt.xticks(range(len(problems)), problems, rotation=45, ha='right')\n",
    "    plt.title('Top 5 Most Common Networking Problems')\n",
    "    plt.xlabel('Problem')\n",
    "    plt.ylabel('Number of Mentions')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('top_problems.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    df.to_csv('networking_posts.csv', index=False)\n",
    "    \n",
    "    with open('networking_problems.txt', 'w') as f:\n",
    "        for problem, count in problem_counter.most_common():\n",
    "            f.write(f\"{problem}: {count}\\n\")\n",
    "    \n",
    "    print(\"Analysis complete! Results saved to:\")\n",
    "    print(\"- networking_posts.csv\")\n",
    "    print(\"- networking_problems.txt\")\n",
    "    print(\"- sentiment_distribution.png\")\n",
    "    print(\"- top_problems.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape Reddit posts...\n",
      "Scraping posts from r/jobs...\n",
      "\n",
      "Fetching hot posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abmir\\AppData\\Local\\Temp\\ipykernel_21236\\2201930006.py:121: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cutoff_date = datetime.utcnow() - timedelta(days=365)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found relevant post 1 (from 2025-03-22)\n",
      "Found relevant post 2 (from 2025-03-22)\n",
      "Found relevant post 3 (from 2025-03-21)\n",
      "Found relevant post 4 (from 2025-03-22)\n",
      "Found relevant post 5 (from 2025-03-21)\n",
      "Found relevant post 6 (from 2025-03-21)\n",
      "Found relevant post 7 (from 2025-03-22)\n",
      "Found relevant post 8 (from 2025-03-22)\n",
      "Found relevant post 9 (from 2025-03-22)\n",
      "Found relevant post 10 (from 2025-03-22)\n",
      "Found relevant post 11 (from 2025-03-22)\n",
      "Found relevant post 12 (from 2025-03-22)\n",
      "Found relevant post 13 (from 2025-03-22)\n",
      "Found relevant post 14 (from 2025-03-22)\n",
      "Found relevant post 15 (from 2025-03-22)\n",
      "Found relevant post 16 (from 2025-03-22)\n",
      "Found relevant post 17 (from 2025-03-22)\n",
      "Found relevant post 18 (from 2025-03-22)\n",
      "Found relevant post 19 (from 2025-03-22)\n",
      "Found relevant post 20 (from 2025-03-21)\n",
      "Found relevant post 21 (from 2025-03-21)\n",
      "Found relevant post 22 (from 2025-03-22)\n",
      "Found relevant post 23 (from 2025-03-22)\n",
      "Found relevant post 24 (from 2025-03-22)\n",
      "Found relevant post 25 (from 2025-03-22)\n",
      "Found relevant post 26 (from 2025-03-22)\n",
      "Found relevant post 27 (from 2025-03-21)\n",
      "Found relevant post 28 (from 2025-03-21)\n",
      "Found relevant post 29 (from 2025-03-20)\n",
      "Found relevant post 30 (from 2025-03-21)\n",
      "Found relevant post 31 (from 2025-03-21)\n",
      "Found relevant post 32 (from 2025-03-21)\n",
      "Found relevant post 33 (from 2025-03-21)\n",
      "Found relevant post 34 (from 2025-03-21)\n",
      "Found relevant post 35 (from 2025-03-21)\n",
      "Found relevant post 36 (from 2025-03-21)\n",
      "Found relevant post 37 (from 2025-03-21)\n",
      "Found relevant post 38 (from 2025-03-21)\n",
      "Found relevant post 39 (from 2025-03-21)\n",
      "Found relevant post 40 (from 2025-03-21)\n",
      "Found relevant post 41 (from 2025-03-21)\n",
      "Found relevant post 42 (from 2025-03-21)\n",
      "Found relevant post 43 (from 2025-03-21)\n",
      "Found relevant post 44 (from 2025-03-21)\n",
      "Found relevant post 45 (from 2025-03-21)\n",
      "Found relevant post 46 (from 2025-03-21)\n",
      "Found relevant post 47 (from 2025-03-21)\n",
      "Found relevant post 48 (from 2025-03-21)\n",
      "Found relevant post 49 (from 2025-03-21)\n",
      "Found relevant post 50 (from 2025-03-21)\n",
      "Found relevant post 51 (from 2025-03-21)\n",
      "Found relevant post 52 (from 2025-03-21)\n",
      "Found relevant post 53 (from 2025-03-20)\n",
      "Found relevant post 54 (from 2025-03-21)\n",
      "Found relevant post 55 (from 2025-03-21)\n",
      "Found relevant post 56 (from 2025-03-21)\n",
      "Found relevant post 57 (from 2025-03-21)\n",
      "Found relevant post 58 (from 2025-03-21)\n",
      "Found relevant post 59 (from 2025-03-21)\n",
      "Found relevant post 60 (from 2025-03-21)\n",
      "Found relevant post 61 (from 2025-03-21)\n",
      "Found relevant post 62 (from 2025-03-21)\n",
      "Found relevant post 63 (from 2025-03-21)\n",
      "Found relevant post 64 (from 2025-03-21)\n",
      "Found relevant post 65 (from 2025-03-21)\n",
      "Found relevant post 66 (from 2025-03-20)\n",
      "Found relevant post 67 (from 2025-03-21)\n",
      "Found relevant post 68 (from 2025-03-21)\n",
      "Found relevant post 69 (from 2025-03-21)\n",
      "Found relevant post 70 (from 2025-03-21)\n",
      "Found relevant post 71 (from 2025-03-21)\n",
      "Found relevant post 72 (from 2025-03-19)\n",
      "Found relevant post 73 (from 2025-03-21)\n",
      "Found relevant post 74 (from 2025-03-21)\n",
      "Found relevant post 75 (from 2025-03-20)\n",
      "Found relevant post 76 (from 2025-03-21)\n",
      "Found relevant post 77 (from 2025-03-21)\n",
      "Found relevant post 78 (from 2025-03-20)\n",
      "Found relevant post 79 (from 2025-03-20)\n",
      "Found relevant post 80 (from 2025-03-20)\n",
      "Found relevant post 81 (from 2025-03-20)\n",
      "Found relevant post 82 (from 2025-03-20)\n",
      "Found relevant post 83 (from 2025-03-21)\n",
      "Found relevant post 84 (from 2025-03-20)\n",
      "Found relevant post 85 (from 2025-03-21)\n",
      "Found relevant post 86 (from 2025-03-20)\n",
      "Found relevant post 87 (from 2025-03-21)\n",
      "Found relevant post 88 (from 2025-03-20)\n",
      "Found relevant post 89 (from 2025-03-20)\n",
      "Found relevant post 90 (from 2025-03-20)\n",
      "Found relevant post 91 (from 2025-03-20)\n",
      "Found relevant post 92 (from 2025-03-20)\n",
      "Found relevant post 93 (from 2025-03-20)\n",
      "Found relevant post 94 (from 2025-03-20)\n",
      "Found relevant post 95 (from 2025-03-20)\n",
      "Found relevant post 96 (from 2025-03-20)\n",
      "Found relevant post 97 (from 2025-03-20)\n",
      "Found relevant post 98 (from 2025-03-20)\n",
      "Found relevant post 99 (from 2025-03-20)\n",
      "Found relevant post 100 (from 2025-03-20)\n",
      "Found relevant post 101 (from 2025-03-20)\n",
      "Found relevant post 102 (from 2025-03-20)\n",
      "Found relevant post 103 (from 2025-03-20)\n",
      "Found relevant post 104 (from 2025-03-20)\n",
      "Found relevant post 105 (from 2025-03-20)\n",
      "Found relevant post 106 (from 2025-03-20)\n",
      "Found relevant post 107 (from 2025-03-20)\n",
      "Found relevant post 108 (from 2025-03-19)\n",
      "Found relevant post 109 (from 2025-03-20)\n",
      "Found relevant post 110 (from 2025-03-20)\n",
      "Found relevant post 111 (from 2025-03-20)\n",
      "Found relevant post 112 (from 2025-03-19)\n",
      "Found relevant post 113 (from 2025-03-20)\n",
      "Found relevant post 114 (from 2025-03-20)\n",
      "Found relevant post 115 (from 2025-03-20)\n",
      "Found relevant post 116 (from 2025-03-20)\n",
      "Found relevant post 117 (from 2025-03-20)\n",
      "Found relevant post 118 (from 2025-03-20)\n",
      "Found relevant post 119 (from 2025-03-20)\n",
      "Found relevant post 120 (from 2025-03-20)\n",
      "Found relevant post 121 (from 2025-03-19)\n",
      "Found relevant post 122 (from 2025-03-20)\n",
      "Found relevant post 123 (from 2025-03-20)\n",
      "Found relevant post 124 (from 2025-03-20)\n",
      "Found relevant post 125 (from 2025-03-20)\n",
      "Found relevant post 126 (from 2025-03-20)\n",
      "Found relevant post 127 (from 2025-03-20)\n",
      "Found relevant post 128 (from 2025-03-20)\n",
      "Found relevant post 129 (from 2025-03-20)\n",
      "Found relevant post 130 (from 2025-03-20)\n",
      "Found relevant post 131 (from 2025-03-20)\n",
      "Found relevant post 132 (from 2025-03-20)\n",
      "Found relevant post 133 (from 2025-03-20)\n",
      "Found relevant post 134 (from 2025-03-20)\n",
      "Found relevant post 135 (from 2025-03-20)\n",
      "Found relevant post 136 (from 2025-03-20)\n",
      "Found relevant post 137 (from 2025-03-19)\n",
      "Found relevant post 138 (from 2025-03-20)\n",
      "Found relevant post 139 (from 2025-03-20)\n",
      "Found relevant post 140 (from 2025-03-20)\n",
      "Found relevant post 141 (from 2025-03-20)\n",
      "Found relevant post 142 (from 2025-03-20)\n",
      "Found relevant post 143 (from 2025-03-19)\n",
      "Found relevant post 144 (from 2025-03-20)\n",
      "Found relevant post 145 (from 2025-03-20)\n",
      "Found relevant post 146 (from 2025-03-20)\n",
      "Found relevant post 147 (from 2025-03-20)\n",
      "Found relevant post 148 (from 2025-03-18)\n",
      "Found relevant post 149 (from 2025-03-19)\n",
      "Found relevant post 150 (from 2025-03-20)\n",
      "Found relevant post 151 (from 2025-03-20)\n",
      "Found relevant post 152 (from 2025-03-19)\n",
      "Found relevant post 153 (from 2025-03-19)\n",
      "Found relevant post 154 (from 2025-03-19)\n",
      "Found relevant post 155 (from 2025-03-19)\n",
      "Found relevant post 156 (from 2025-03-19)\n",
      "Found relevant post 157 (from 2025-03-19)\n",
      "Found relevant post 158 (from 2025-03-19)\n",
      "Found relevant post 159 (from 2025-03-19)\n",
      "Found relevant post 160 (from 2025-03-19)\n",
      "Found relevant post 161 (from 2025-03-19)\n",
      "Found relevant post 162 (from 2025-03-19)\n",
      "Found relevant post 163 (from 2025-03-18)\n",
      "Found relevant post 164 (from 2025-03-19)\n",
      "Found relevant post 165 (from 2025-03-19)\n",
      "Found relevant post 166 (from 2025-03-19)\n",
      "Found relevant post 167 (from 2025-03-19)\n",
      "Found relevant post 168 (from 2025-03-19)\n",
      "Found relevant post 169 (from 2025-03-19)\n",
      "Found relevant post 170 (from 2025-03-19)\n",
      "Found relevant post 171 (from 2025-03-19)\n",
      "Found relevant post 172 (from 2025-03-19)\n",
      "Found relevant post 173 (from 2025-03-19)\n",
      "Found relevant post 174 (from 2025-03-19)\n",
      "Found relevant post 175 (from 2025-03-19)\n",
      "Found relevant post 176 (from 2025-03-19)\n",
      "Found relevant post 177 (from 2025-03-19)\n",
      "Found relevant post 178 (from 2025-03-18)\n",
      "Found relevant post 179 (from 2025-03-19)\n",
      "Found relevant post 180 (from 2025-03-19)\n",
      "Found relevant post 181 (from 2025-03-19)\n",
      "Found relevant post 182 (from 2025-03-19)\n",
      "Found relevant post 183 (from 2025-03-19)\n",
      "Found relevant post 184 (from 2025-03-19)\n",
      "Found relevant post 185 (from 2025-03-19)\n",
      "Found relevant post 186 (from 2025-03-19)\n",
      "Found relevant post 187 (from 2025-03-19)\n",
      "Found relevant post 188 (from 2025-03-18)\n",
      "Found relevant post 189 (from 2025-03-19)\n",
      "Found relevant post 190 (from 2025-03-19)\n",
      "Found relevant post 191 (from 2025-03-19)\n",
      "Found relevant post 192 (from 2025-03-19)\n",
      "Found relevant post 193 (from 2025-03-19)\n",
      "Found relevant post 194 (from 2025-03-19)\n",
      "Found relevant post 195 (from 2025-03-19)\n",
      "Found relevant post 196 (from 2025-03-19)\n",
      "Found relevant post 197 (from 2025-03-19)\n",
      "Found relevant post 198 (from 2025-03-19)\n",
      "Found relevant post 199 (from 2025-03-19)\n",
      "Found relevant post 200 (from 2025-03-19)\n",
      "Found relevant post 201 (from 2025-03-19)\n",
      "Found relevant post 202 (from 2025-03-19)\n",
      "Found relevant post 203 (from 2025-03-19)\n",
      "Found relevant post 204 (from 2025-03-19)\n",
      "Found relevant post 205 (from 2025-03-19)\n",
      "Found relevant post 206 (from 2025-03-19)\n",
      "Found relevant post 207 (from 2025-03-19)\n",
      "Found relevant post 208 (from 2025-03-19)\n",
      "Found relevant post 209 (from 2025-03-19)\n",
      "Found relevant post 210 (from 2025-03-19)\n",
      "Found relevant post 211 (from 2025-03-19)\n",
      "Found relevant post 212 (from 2025-03-17)\n",
      "Found relevant post 213 (from 2025-03-19)\n",
      "Found relevant post 214 (from 2025-03-19)\n",
      "Found relevant post 215 (from 2025-03-19)\n",
      "Found relevant post 216 (from 2025-03-19)\n",
      "Found relevant post 217 (from 2025-03-19)\n",
      "Found relevant post 218 (from 2025-03-19)\n",
      "Found relevant post 219 (from 2025-03-19)\n",
      "Found relevant post 220 (from 2025-03-18)\n",
      "Found relevant post 221 (from 2025-03-18)\n",
      "Found relevant post 222 (from 2025-03-18)\n",
      "Found relevant post 223 (from 2025-03-19)\n",
      "Found relevant post 224 (from 2025-03-17)\n",
      "Found relevant post 225 (from 2025-03-19)\n",
      "Found relevant post 226 (from 2025-03-18)\n",
      "Found relevant post 227 (from 2025-03-19)\n",
      "Found relevant post 228 (from 2025-03-19)\n",
      "Found relevant post 229 (from 2025-03-19)\n",
      "Found relevant post 230 (from 2025-03-18)\n",
      "Found relevant post 231 (from 2025-03-18)\n",
      "Found relevant post 232 (from 2025-03-18)\n",
      "Found relevant post 233 (from 2025-03-18)\n",
      "Found relevant post 234 (from 2025-03-18)\n",
      "Found relevant post 235 (from 2025-03-18)\n",
      "Found relevant post 236 (from 2025-03-18)\n",
      "Found relevant post 237 (from 2025-03-18)\n",
      "Found relevant post 238 (from 2025-03-18)\n",
      "Found relevant post 239 (from 2025-03-17)\n",
      "Found relevant post 240 (from 2025-03-18)\n",
      "Found relevant post 241 (from 2025-03-17)\n",
      "Found relevant post 242 (from 2025-03-18)\n",
      "Found relevant post 243 (from 2025-03-18)\n",
      "Found relevant post 244 (from 2025-03-18)\n",
      "Found relevant post 245 (from 2025-03-18)\n",
      "Found relevant post 246 (from 2025-03-18)\n",
      "Found relevant post 247 (from 2025-03-18)\n",
      "Found relevant post 248 (from 2025-03-18)\n",
      "Found relevant post 249 (from 2025-03-18)\n",
      "Found relevant post 250 (from 2025-03-18)\n",
      "Found relevant post 251 (from 2025-03-18)\n",
      "Found relevant post 252 (from 2025-03-18)\n",
      "Found relevant post 253 (from 2025-03-18)\n",
      "Found relevant post 254 (from 2025-03-18)\n",
      "Found relevant post 255 (from 2025-03-18)\n",
      "Found relevant post 256 (from 2025-03-18)\n",
      "Found relevant post 257 (from 2025-03-18)\n",
      "Found relevant post 258 (from 2025-03-18)\n",
      "Found relevant post 259 (from 2025-03-18)\n",
      "Found relevant post 260 (from 2025-03-18)\n",
      "Found relevant post 261 (from 2025-03-18)\n",
      "\n",
      "Fetching top posts...\n",
      "Found relevant post 262 (from 2025-02-07)\n",
      "Found relevant post 263 (from 2025-02-11)\n",
      "Found relevant post 264 (from 2024-12-05)\n",
      "Found relevant post 265 (from 2024-08-13)\n",
      "Found relevant post 266 (from 2025-01-07)\n",
      "Found relevant post 267 (from 2024-08-12)\n",
      "Found relevant post 268 (from 2024-08-23)\n",
      "Found relevant post 269 (from 2024-08-28)\n",
      "Found relevant post 270 (from 2024-11-05)\n",
      "Found relevant post 271 (from 2024-08-14)\n",
      "Found relevant post 272 (from 2025-01-28)\n",
      "Found relevant post 273 (from 2025-02-05)\n",
      "Found relevant post 274 (from 2024-08-16)\n",
      "Found relevant post 275 (from 2025-02-23)\n",
      "Found relevant post 276 (from 2024-04-21)\n",
      "Found relevant post 277 (from 2024-12-11)\n",
      "Found relevant post 278 (from 2024-07-26)\n",
      "Found relevant post 279 (from 2025-02-18)\n",
      "Found relevant post 280 (from 2024-04-10)\n",
      "Found relevant post 281 (from 2024-04-06)\n",
      "Found relevant post 282 (from 2025-02-06)\n",
      "Found relevant post 283 (from 2025-02-13)\n",
      "Found relevant post 284 (from 2024-05-20)\n",
      "Found relevant post 285 (from 2025-03-07)\n",
      "Found relevant post 286 (from 2024-09-30)\n",
      "Found relevant post 287 (from 2024-04-02)\n",
      "Found relevant post 288 (from 2024-08-26)\n",
      "Found relevant post 289 (from 2024-09-22)\n",
      "Found relevant post 290 (from 2024-04-22)\n",
      "Found relevant post 291 (from 2024-04-25)\n",
      "Found relevant post 292 (from 2024-09-25)\n",
      "Found relevant post 293 (from 2024-04-16)\n",
      "Found relevant post 294 (from 2025-03-18)\n",
      "Found relevant post 295 (from 2024-09-10)\n",
      "Found relevant post 296 (from 2024-07-12)\n",
      "Found relevant post 297 (from 2024-11-06)\n",
      "Found relevant post 298 (from 2025-02-27)\n",
      "Found relevant post 299 (from 2024-05-18)\n",
      "Found relevant post 300 (from 2024-06-02)\n",
      "Found relevant post 301 (from 2025-01-18)\n",
      "Found relevant post 302 (from 2025-03-07)\n",
      "Found relevant post 303 (from 2024-09-25)\n",
      "Found relevant post 304 (from 2024-09-04)\n",
      "Found relevant post 305 (from 2024-10-25)\n",
      "Found relevant post 306 (from 2025-03-06)\n",
      "Found relevant post 307 (from 2024-11-23)\n",
      "Found relevant post 308 (from 2024-04-29)\n",
      "Found relevant post 309 (from 2024-05-08)\n",
      "Found relevant post 310 (from 2024-06-05)\n",
      "Found relevant post 311 (from 2025-03-19)\n",
      "Found relevant post 312 (from 2024-08-01)\n",
      "Found relevant post 313 (from 2024-07-08)\n",
      "Found relevant post 314 (from 2025-02-20)\n",
      "Found relevant post 315 (from 2025-03-14)\n",
      "Found relevant post 316 (from 2024-05-27)\n",
      "Found relevant post 317 (from 2024-08-05)\n",
      "Found relevant post 318 (from 2024-08-16)\n",
      "Found relevant post 319 (from 2025-03-06)\n",
      "Found relevant post 320 (from 2024-10-25)\n",
      "Found relevant post 321 (from 2024-08-25)\n",
      "Found relevant post 322 (from 2024-05-10)\n",
      "Found relevant post 323 (from 2024-12-30)\n",
      "Found relevant post 324 (from 2025-03-13)\n",
      "Found relevant post 325 (from 2025-02-04)\n",
      "Found relevant post 326 (from 2025-02-10)\n",
      "Found relevant post 327 (from 2024-05-10)\n",
      "Found relevant post 328 (from 2024-12-13)\n",
      "Found relevant post 329 (from 2024-10-28)\n",
      "Found relevant post 330 (from 2025-01-26)\n",
      "Found relevant post 331 (from 2024-05-01)\n",
      "Found relevant post 332 (from 2024-10-04)\n",
      "Found relevant post 333 (from 2024-12-09)\n",
      "Found relevant post 334 (from 2024-04-26)\n",
      "Found relevant post 335 (from 2024-10-30)\n",
      "Found relevant post 336 (from 2024-06-12)\n",
      "Found relevant post 337 (from 2025-02-23)\n",
      "Found relevant post 338 (from 2024-03-26)\n",
      "Found relevant post 339 (from 2024-05-05)\n",
      "Found relevant post 340 (from 2025-02-10)\n",
      "Found relevant post 341 (from 2024-04-11)\n",
      "Found relevant post 342 (from 2024-08-13)\n",
      "Found relevant post 343 (from 2024-09-27)\n",
      "Found relevant post 344 (from 2024-07-29)\n",
      "Found relevant post 345 (from 2024-08-28)\n",
      "Found relevant post 346 (from 2024-09-13)\n",
      "Found relevant post 347 (from 2024-08-23)\n",
      "Found relevant post 348 (from 2025-02-13)\n",
      "Found relevant post 349 (from 2025-02-19)\n",
      "Found relevant post 350 (from 2024-08-19)\n",
      "Found relevant post 351 (from 2024-09-23)\n",
      "Found relevant post 352 (from 2025-02-17)\n",
      "Found relevant post 353 (from 2025-02-26)\n",
      "Found relevant post 354 (from 2024-11-12)\n",
      "Found relevant post 355 (from 2025-02-08)\n",
      "Found relevant post 356 (from 2024-05-15)\n",
      "Found relevant post 357 (from 2024-05-28)\n",
      "Found relevant post 358 (from 2024-05-10)\n",
      "Found relevant post 359 (from 2024-09-29)\n",
      "Found relevant post 360 (from 2024-10-27)\n",
      "Found relevant post 361 (from 2024-05-23)\n",
      "Found relevant post 362 (from 2024-06-04)\n",
      "Found relevant post 363 (from 2025-03-10)\n",
      "Found relevant post 364 (from 2024-04-29)\n",
      "Found relevant post 365 (from 2024-06-09)\n",
      "Found relevant post 366 (from 2024-11-05)\n",
      "Found relevant post 367 (from 2025-01-03)\n",
      "Found relevant post 368 (from 2024-12-22)\n",
      "\n",
      "Fetching new posts...\n",
      "Found relevant post 369 (from 2025-03-22)\n",
      "Found relevant post 370 (from 2025-03-22)\n",
      "Found relevant post 371 (from 2025-03-22)\n",
      "Found relevant post 372 (from 2025-03-22)\n",
      "Found relevant post 373 (from 2025-03-22)\n",
      "Found relevant post 374 (from 2025-03-22)\n",
      "Found relevant post 375 (from 2025-03-22)\n",
      "Found relevant post 376 (from 2025-03-22)\n",
      "Found relevant post 377 (from 2025-03-22)\n",
      "Found relevant post 378 (from 2025-03-22)\n",
      "Found relevant post 379 (from 2025-03-22)\n",
      "Found relevant post 380 (from 2025-03-22)\n",
      "Found relevant post 381 (from 2025-03-22)\n",
      "Found relevant post 382 (from 2025-03-22)\n",
      "Found relevant post 383 (from 2025-03-22)\n",
      "Found relevant post 384 (from 2025-03-22)\n",
      "Found relevant post 385 (from 2025-03-22)\n",
      "Found relevant post 386 (from 2025-03-22)\n",
      "Found relevant post 387 (from 2025-03-22)\n",
      "Found relevant post 388 (from 2025-03-22)\n",
      "Found relevant post 389 (from 2025-03-22)\n",
      "Found relevant post 390 (from 2025-03-21)\n",
      "Found relevant post 391 (from 2025-03-21)\n",
      "Found relevant post 392 (from 2025-03-21)\n",
      "Found relevant post 393 (from 2025-03-21)\n",
      "Found relevant post 394 (from 2025-03-21)\n",
      "Found relevant post 395 (from 2025-03-21)\n",
      "Found relevant post 396 (from 2025-03-21)\n",
      "Found relevant post 397 (from 2025-03-21)\n",
      "Found relevant post 398 (from 2025-03-21)\n",
      "Found relevant post 399 (from 2025-03-21)\n",
      "Found relevant post 400 (from 2025-03-21)\n",
      "Found relevant post 401 (from 2025-03-21)\n",
      "Found relevant post 402 (from 2025-03-21)\n",
      "Found relevant post 403 (from 2025-03-21)\n",
      "Found relevant post 404 (from 2025-03-21)\n",
      "Found relevant post 405 (from 2025-03-21)\n",
      "Found relevant post 406 (from 2025-03-21)\n",
      "Found relevant post 407 (from 2025-03-21)\n",
      "Found relevant post 408 (from 2025-03-21)\n",
      "Found relevant post 409 (from 2025-03-21)\n",
      "Found relevant post 410 (from 2025-03-21)\n",
      "Found relevant post 411 (from 2025-03-21)\n",
      "Found relevant post 412 (from 2025-03-21)\n",
      "Found relevant post 413 (from 2025-03-21)\n",
      "Found relevant post 414 (from 2025-03-21)\n",
      "Found relevant post 415 (from 2025-03-21)\n",
      "Found relevant post 416 (from 2025-03-21)\n",
      "Found relevant post 417 (from 2025-03-21)\n",
      "Found relevant post 418 (from 2025-03-21)\n",
      "Found relevant post 419 (from 2025-03-21)\n",
      "Found relevant post 420 (from 2025-03-21)\n",
      "Found relevant post 421 (from 2025-03-21)\n",
      "Found relevant post 422 (from 2025-03-21)\n",
      "Found relevant post 423 (from 2025-03-21)\n",
      "Found relevant post 424 (from 2025-03-21)\n",
      "Found relevant post 425 (from 2025-03-21)\n",
      "Found relevant post 426 (from 2025-03-21)\n",
      "Found relevant post 427 (from 2025-03-21)\n",
      "Found relevant post 428 (from 2025-03-21)\n",
      "Found relevant post 429 (from 2025-03-21)\n",
      "Found relevant post 430 (from 2025-03-21)\n",
      "Found relevant post 431 (from 2025-03-21)\n",
      "Found relevant post 432 (from 2025-03-21)\n",
      "Found relevant post 433 (from 2025-03-21)\n",
      "Found relevant post 434 (from 2025-03-21)\n",
      "Found relevant post 435 (from 2025-03-21)\n",
      "Found relevant post 436 (from 2025-03-21)\n",
      "Found relevant post 437 (from 2025-03-21)\n",
      "Found relevant post 438 (from 2025-03-21)\n",
      "Found relevant post 439 (from 2025-03-21)\n",
      "Found relevant post 440 (from 2025-03-21)\n",
      "Found relevant post 441 (from 2025-03-21)\n",
      "Found relevant post 442 (from 2025-03-21)\n",
      "Found relevant post 443 (from 2025-03-21)\n",
      "Found relevant post 444 (from 2025-03-20)\n",
      "Found relevant post 445 (from 2025-03-20)\n",
      "Found relevant post 446 (from 2025-03-20)\n",
      "Found relevant post 447 (from 2025-03-20)\n",
      "Found relevant post 448 (from 2025-03-20)\n",
      "Found relevant post 449 (from 2025-03-20)\n",
      "Found relevant post 450 (from 2025-03-20)\n",
      "Found relevant post 451 (from 2025-03-20)\n",
      "Found relevant post 452 (from 2025-03-20)\n",
      "Found relevant post 453 (from 2025-03-20)\n",
      "Found relevant post 454 (from 2025-03-20)\n",
      "Found relevant post 455 (from 2025-03-20)\n",
      "Found relevant post 456 (from 2025-03-20)\n",
      "Found relevant post 457 (from 2025-03-20)\n",
      "Found relevant post 458 (from 2025-03-20)\n",
      "Found relevant post 459 (from 2025-03-20)\n",
      "Found relevant post 460 (from 2025-03-20)\n",
      "Found relevant post 461 (from 2025-03-20)\n",
      "Found relevant post 462 (from 2025-03-20)\n",
      "Found relevant post 463 (from 2025-03-20)\n",
      "Found relevant post 464 (from 2025-03-20)\n",
      "Found relevant post 465 (from 2025-03-20)\n",
      "Found relevant post 466 (from 2025-03-20)\n",
      "Found relevant post 467 (from 2025-03-20)\n",
      "Found relevant post 468 (from 2025-03-20)\n",
      "Found relevant post 469 (from 2025-03-20)\n",
      "Found relevant post 470 (from 2025-03-20)\n",
      "Found relevant post 471 (from 2025-03-20)\n",
      "Found relevant post 472 (from 2025-03-20)\n",
      "Found relevant post 473 (from 2025-03-20)\n",
      "Found relevant post 474 (from 2025-03-20)\n",
      "Found relevant post 475 (from 2025-03-20)\n",
      "Found relevant post 476 (from 2025-03-20)\n",
      "Found relevant post 477 (from 2025-03-20)\n",
      "Found relevant post 478 (from 2025-03-20)\n",
      "Found relevant post 479 (from 2025-03-20)\n",
      "Found relevant post 480 (from 2025-03-20)\n",
      "Found relevant post 481 (from 2025-03-20)\n",
      "Found relevant post 482 (from 2025-03-20)\n",
      "Found relevant post 483 (from 2025-03-20)\n",
      "Found relevant post 484 (from 2025-03-20)\n",
      "Found relevant post 485 (from 2025-03-20)\n",
      "Found relevant post 486 (from 2025-03-20)\n",
      "Found relevant post 487 (from 2025-03-20)\n",
      "Found relevant post 488 (from 2025-03-20)\n",
      "Found relevant post 489 (from 2025-03-20)\n",
      "Found relevant post 490 (from 2025-03-20)\n",
      "Found relevant post 491 (from 2025-03-20)\n",
      "Found relevant post 492 (from 2025-03-20)\n",
      "Found relevant post 493 (from 2025-03-20)\n",
      "Found relevant post 494 (from 2025-03-20)\n",
      "Found relevant post 495 (from 2025-03-20)\n",
      "Found relevant post 496 (from 2025-03-20)\n",
      "Found relevant post 497 (from 2025-03-20)\n",
      "Found relevant post 498 (from 2025-03-20)\n",
      "Found relevant post 499 (from 2025-03-20)\n",
      "Found relevant post 500 (from 2025-03-20)\n",
      "Found relevant post 501 (from 2025-03-20)\n",
      "Found relevant post 502 (from 2025-03-20)\n",
      "Found relevant post 503 (from 2025-03-20)\n",
      "Found relevant post 504 (from 2025-03-20)\n",
      "Found relevant post 505 (from 2025-03-20)\n",
      "Found relevant post 506 (from 2025-03-20)\n",
      "Found relevant post 507 (from 2025-03-20)\n",
      "Found relevant post 508 (from 2025-03-20)\n",
      "Found relevant post 509 (from 2025-03-20)\n",
      "Found relevant post 510 (from 2025-03-20)\n",
      "Found relevant post 511 (from 2025-03-20)\n",
      "Found relevant post 512 (from 2025-03-19)\n",
      "Found relevant post 513 (from 2025-03-19)\n",
      "Found relevant post 514 (from 2025-03-19)\n",
      "Found relevant post 515 (from 2025-03-19)\n",
      "Found relevant post 516 (from 2025-03-19)\n",
      "Found relevant post 517 (from 2025-03-19)\n",
      "Found relevant post 518 (from 2025-03-19)\n",
      "Found relevant post 519 (from 2025-03-19)\n",
      "Found relevant post 520 (from 2025-03-19)\n",
      "Found relevant post 521 (from 2025-03-19)\n",
      "Found relevant post 522 (from 2025-03-19)\n",
      "Found relevant post 523 (from 2025-03-19)\n",
      "Found relevant post 524 (from 2025-03-19)\n",
      "Found relevant post 525 (from 2025-03-19)\n",
      "Found relevant post 526 (from 2025-03-19)\n",
      "Found relevant post 527 (from 2025-03-19)\n",
      "Found relevant post 528 (from 2025-03-19)\n",
      "Found relevant post 529 (from 2025-03-19)\n",
      "Found relevant post 530 (from 2025-03-19)\n",
      "Found relevant post 531 (from 2025-03-19)\n",
      "Found relevant post 532 (from 2025-03-19)\n",
      "Found relevant post 533 (from 2025-03-19)\n",
      "Found relevant post 534 (from 2025-03-19)\n",
      "Found relevant post 535 (from 2025-03-19)\n",
      "Found relevant post 536 (from 2025-03-19)\n",
      "Found relevant post 537 (from 2025-03-19)\n",
      "Found relevant post 538 (from 2025-03-19)\n",
      "Found relevant post 539 (from 2025-03-19)\n",
      "Found relevant post 540 (from 2025-03-19)\n",
      "Found relevant post 541 (from 2025-03-19)\n",
      "Found relevant post 542 (from 2025-03-19)\n",
      "Found relevant post 543 (from 2025-03-19)\n",
      "Found relevant post 544 (from 2025-03-19)\n",
      "Found relevant post 545 (from 2025-03-19)\n",
      "Found relevant post 546 (from 2025-03-19)\n",
      "Found relevant post 547 (from 2025-03-19)\n",
      "Found relevant post 548 (from 2025-03-19)\n",
      "Found relevant post 549 (from 2025-03-19)\n",
      "Found relevant post 550 (from 2025-03-19)\n",
      "Found relevant post 551 (from 2025-03-19)\n",
      "Found relevant post 552 (from 2025-03-19)\n",
      "Found relevant post 553 (from 2025-03-19)\n",
      "Found relevant post 554 (from 2025-03-19)\n",
      "Found relevant post 555 (from 2025-03-19)\n",
      "Found relevant post 556 (from 2025-03-19)\n",
      "Found relevant post 557 (from 2025-03-19)\n",
      "Found relevant post 558 (from 2025-03-19)\n",
      "Found relevant post 559 (from 2025-03-19)\n",
      "Found relevant post 560 (from 2025-03-19)\n",
      "Found relevant post 561 (from 2025-03-19)\n",
      "Found relevant post 562 (from 2025-03-19)\n",
      "Found relevant post 563 (from 2025-03-19)\n",
      "Found relevant post 564 (from 2025-03-19)\n",
      "Found relevant post 565 (from 2025-03-19)\n",
      "Found relevant post 566 (from 2025-03-19)\n",
      "Found relevant post 567 (from 2025-03-19)\n",
      "Found relevant post 568 (from 2025-03-19)\n",
      "Found relevant post 569 (from 2025-03-19)\n",
      "Found relevant post 570 (from 2025-03-19)\n",
      "Found relevant post 571 (from 2025-03-19)\n",
      "Found relevant post 572 (from 2025-03-19)\n",
      "Found relevant post 573 (from 2025-03-19)\n",
      "Found relevant post 574 (from 2025-03-19)\n",
      "Found relevant post 575 (from 2025-03-19)\n",
      "Found relevant post 576 (from 2025-03-19)\n",
      "Found relevant post 577 (from 2025-03-19)\n",
      "Found relevant post 578 (from 2025-03-19)\n",
      "Found relevant post 579 (from 2025-03-19)\n",
      "Found relevant post 580 (from 2025-03-19)\n",
      "Found relevant post 581 (from 2025-03-19)\n",
      "Found relevant post 582 (from 2025-03-19)\n",
      "Found relevant post 583 (from 2025-03-19)\n",
      "Found relevant post 584 (from 2025-03-19)\n",
      "Found relevant post 585 (from 2025-03-19)\n",
      "Found relevant post 586 (from 2025-03-19)\n",
      "Found relevant post 587 (from 2025-03-19)\n",
      "Found relevant post 588 (from 2025-03-18)\n",
      "Found relevant post 589 (from 2025-03-18)\n",
      "Found relevant post 590 (from 2025-03-18)\n",
      "Found relevant post 591 (from 2025-03-18)\n",
      "Found relevant post 592 (from 2025-03-18)\n",
      "Found relevant post 593 (from 2025-03-18)\n",
      "Found relevant post 594 (from 2025-03-18)\n",
      "Found relevant post 595 (from 2025-03-18)\n",
      "Found relevant post 596 (from 2025-03-18)\n",
      "Found relevant post 597 (from 2025-03-18)\n",
      "Found relevant post 598 (from 2025-03-18)\n",
      "Found relevant post 599 (from 2025-03-18)\n",
      "Found relevant post 600 (from 2025-03-18)\n",
      "Found relevant post 601 (from 2025-03-18)\n",
      "Found relevant post 602 (from 2025-03-18)\n",
      "Found relevant post 603 (from 2025-03-18)\n",
      "Found relevant post 604 (from 2025-03-18)\n",
      "Found relevant post 605 (from 2025-03-18)\n",
      "Found relevant post 606 (from 2025-03-18)\n",
      "Found relevant post 607 (from 2025-03-18)\n",
      "Found relevant post 608 (from 2025-03-18)\n",
      "Found relevant post 609 (from 2025-03-18)\n",
      "Found relevant post 610 (from 2025-03-18)\n",
      "Found relevant post 611 (from 2025-03-18)\n",
      "Found relevant post 612 (from 2025-03-18)\n",
      "Found relevant post 613 (from 2025-03-18)\n",
      "Found relevant post 614 (from 2025-03-18)\n",
      "Found relevant post 615 (from 2025-03-18)\n",
      "Found relevant post 616 (from 2025-03-18)\n",
      "Found relevant post 617 (from 2025-03-18)\n",
      "Found relevant post 618 (from 2025-03-18)\n",
      "Found relevant post 619 (from 2025-03-18)\n",
      "Found relevant post 620 (from 2025-03-18)\n",
      "Found relevant post 621 (from 2025-03-18)\n",
      "Found relevant post 622 (from 2025-03-18)\n",
      "Found relevant post 623 (from 2025-03-18)\n",
      "Found relevant post 624 (from 2025-03-18)\n",
      "Found relevant post 625 (from 2025-03-18)\n",
      "Found relevant post 626 (from 2025-03-18)\n",
      "Found relevant post 627 (from 2025-03-18)\n",
      "Found relevant post 628 (from 2025-03-18)\n",
      "Found relevant post 629 (from 2025-03-18)\n",
      "Found relevant post 630 (from 2025-03-18)\n",
      "Found relevant post 631 (from 2025-03-18)\n",
      "Found relevant post 632 (from 2025-03-18)\n",
      "Found relevant post 633 (from 2025-03-18)\n",
      "Found relevant post 634 (from 2025-03-18)\n",
      "Found relevant post 635 (from 2025-03-18)\n",
      "Found relevant post 636 (from 2025-03-18)\n",
      "Found relevant post 637 (from 2025-03-18)\n",
      "Found relevant post 638 (from 2025-03-18)\n",
      "Found relevant post 639 (from 2025-03-18)\n",
      "Found relevant post 640 (from 2025-03-18)\n",
      "Found relevant post 641 (from 2025-03-18)\n",
      "Found relevant post 642 (from 2025-03-18)\n",
      "Found relevant post 643 (from 2025-03-18)\n",
      "Found relevant post 644 (from 2025-03-18)\n",
      "Found relevant post 645 (from 2025-03-17)\n",
      "Found relevant post 646 (from 2025-03-17)\n",
      "Found relevant post 647 (from 2025-03-17)\n",
      "Found relevant post 648 (from 2025-03-17)\n",
      "Found relevant post 649 (from 2025-03-17)\n",
      "Found relevant post 650 (from 2025-03-17)\n",
      "Found relevant post 651 (from 2025-03-17)\n",
      "Found relevant post 652 (from 2025-03-17)\n",
      "Found relevant post 653 (from 2025-03-17)\n",
      "Found relevant post 654 (from 2025-03-17)\n",
      "Found relevant post 655 (from 2025-03-17)\n",
      "Found relevant post 656 (from 2025-03-17)\n",
      "Found relevant post 657 (from 2025-03-17)\n",
      "Found relevant post 658 (from 2025-03-17)\n",
      "Found relevant post 659 (from 2025-03-17)\n",
      "Found relevant post 660 (from 2025-03-17)\n",
      "Found relevant post 661 (from 2025-03-17)\n",
      "Found relevant post 662 (from 2025-03-17)\n",
      "Found relevant post 663 (from 2025-03-17)\n",
      "Found relevant post 664 (from 2025-03-17)\n",
      "Found relevant post 665 (from 2025-03-17)\n",
      "Found relevant post 666 (from 2025-03-17)\n",
      "Found relevant post 667 (from 2025-03-17)\n",
      "Found relevant post 668 (from 2025-03-17)\n",
      "Found relevant post 669 (from 2025-03-17)\n",
      "Found relevant post 670 (from 2025-03-17)\n",
      "Found relevant post 671 (from 2025-03-17)\n",
      "Found relevant post 672 (from 2025-03-17)\n",
      "Found relevant post 673 (from 2025-03-17)\n",
      "Found relevant post 674 (from 2025-03-17)\n",
      "Found relevant post 675 (from 2025-03-17)\n",
      "Found relevant post 676 (from 2025-03-17)\n",
      "Found relevant post 677 (from 2025-03-17)\n",
      "Found relevant post 678 (from 2025-03-17)\n",
      "Found relevant post 679 (from 2025-03-17)\n",
      "Found relevant post 680 (from 2025-03-17)\n",
      "Found relevant post 681 (from 2025-03-17)\n",
      "Found relevant post 682 (from 2025-03-17)\n",
      "Found relevant post 683 (from 2025-03-17)\n",
      "Found relevant post 684 (from 2025-03-17)\n",
      "Found relevant post 685 (from 2025-03-17)\n",
      "Found relevant post 686 (from 2025-03-17)\n",
      "Found relevant post 687 (from 2025-03-17)\n",
      "Found relevant post 688 (from 2025-03-17)\n",
      "Found relevant post 689 (from 2025-03-17)\n",
      "Found relevant post 690 (from 2025-03-17)\n",
      "Found relevant post 691 (from 2025-03-17)\n",
      "Found relevant post 692 (from 2025-03-17)\n",
      "Found relevant post 693 (from 2025-03-17)\n",
      "Found relevant post 694 (from 2025-03-17)\n",
      "Found relevant post 695 (from 2025-03-17)\n",
      "Found relevant post 696 (from 2025-03-17)\n",
      "Found relevant post 697 (from 2025-03-17)\n",
      "Found relevant post 698 (from 2025-03-17)\n",
      "Found relevant post 699 (from 2025-03-17)\n",
      "\n",
      "Total relevant posts found: 699\n",
      "\n",
      "Analyzing topics...\n",
      "\n",
      "Analyzing bigrams...\n",
      "\n",
      "Top Topics:\n",
      "===========\n",
      "   Topic                                             Top Words\n",
      " Topic 1                  hard, work, time, work hard, working\n",
      " Topic 2                        problem, said, sb, asked, sure\n",
      " Topic 3                    issue, long, mentioned, began, low\n",
      " Topic 4                 know, want, jobs, honestly, situation\n",
      " Topic 5      hate, hate feeling, hate job, feeling, coworkers\n",
      " Topic 6 issues, health, perfect, health issues, outlook email\n",
      " Topic 7                   job, getting, tired, difficult, new\n",
      " Topic 8              confused, interview, nervous, little, im\n",
      " Topic 9                   feel, like, really, feel like, boss\n",
      "Topic 10                   scared, going, face, years, despite\n",
      "\n",
      "Top Bigrams:\n",
      "============\n",
      "Bigram  Count\n",
      "   i ’    252\n",
      "   ’ m    169\n",
      " and i    153\n",
      "   , i    133\n",
      "   ’ s    131\n",
      "   ’ t    130\n",
      "  i 'm    126\n",
      " , but    116\n",
      " but i    114\n",
      " i was    101\n",
      "\n",
      "Saving results...\n",
      "\n",
      "Analysis complete! Results saved to:\n",
      "- job_hunting_posts.csv\n",
      "- topic_analysis.csv\n",
      "- bigram_analysis.csv\n",
      "- complaint_analysis.txt\n",
      "- complaint_bigrams.png\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"nNt1apRLcond1DKKcfhMJg\",\n",
    "    client_secret=\"OYprE9hUaZriB4lRixxH6iq3duoZXw\",\n",
    "    user_agent=\"script:job_hunting_analysis:v1.0 (by /u/YOUR_USERNAME)\"\n",
    ")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_complaints(text):\n",
    "    # Common complaint indicators\n",
    "    complaint_indicators = [\n",
    "        'difficult', 'hard', 'trouble', 'struggle', 'problem', 'issue',\n",
    "        'challenge', 'frustrated', 'anxious', 'nervous', 'afraid', 'scared',\n",
    "        'don\\'t know', 'confused', 'overwhelmed', 'stuck', 'hate', 'terrible',\n",
    "        'awful', 'worst', 'impossible', 'ridiculous', 'annoying', 'tired of',\n",
    "        'sick of', 'fed up with', 'exhausted', 'drained', 'burned out'\n",
    "    ]\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split('[.!?]+', text.lower())\n",
    "    \n",
    "    # Extract sentences containing complaint indicators\n",
    "    complaints = []\n",
    "    for sentence in sentences:\n",
    "        if any(indicator in sentence for indicator in complaint_indicators):\n",
    "            complaints.append(sentence.strip())\n",
    "    \n",
    "    return complaints\n",
    "\n",
    "def get_top_ngrams(texts, n=2, top_n=10):\n",
    "    # Create n-grams\n",
    "    ngram_list = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        ngram_list.extend(list(ngrams(tokens, n)))\n",
    "    \n",
    "    # Count n-grams\n",
    "    ngram_counts = Counter(ngram_list)\n",
    "    \n",
    "    # Get top n-grams\n",
    "    return ngram_counts.most_common(top_n)\n",
    "\n",
    "def analyze_topics(texts, n_topics=10, n_words=5):\n",
    "    # Preprocess texts\n",
    "    processed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),  # Include both unigrams and bigrams\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    dtm = vectorizer.fit_transform(processed_texts)\n",
    "    \n",
    "    # Apply Non-negative Matrix Factorization for topic modeling\n",
    "    nmf = NMF(n_components=n_topics, random_state=42)\n",
    "    nmf.fit(dtm)\n",
    "    \n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top words for each topic\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_words-1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(top_words)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def scrape_reddit_posts():\n",
    "    posts = []\n",
    "    complaints = []\n",
    "    subreddit = reddit.subreddit('jobs')\n",
    "    \n",
    "    print(f\"Scraping posts from r/jobs...\")\n",
    "    \n",
    "    # Get posts from the last year\n",
    "    cutoff_date = datetime.utcnow() - timedelta(days=365)\n",
    "    \n",
    "    # Get posts from different time periods to ensure we get enough data\n",
    "    for time_filter in ['hot', 'top', 'new']:\n",
    "        print(f\"\\nFetching {time_filter} posts...\")\n",
    "        for post in getattr(subreddit, time_filter)(limit=None):\n",
    "            # Check if post is from within our time window\n",
    "            post_date = datetime.fromtimestamp(post.created_utc)\n",
    "            if post_date < cutoff_date:\n",
    "                continue\n",
    "            \n",
    "            # Extract complaints\n",
    "            post_complaints = extract_complaints(post.title) + extract_complaints(post.selftext)\n",
    "            \n",
    "            if post_complaints:  # Only include posts with complaints\n",
    "                posts.append({\n",
    "                    'title': post.title,\n",
    "                    'text': post.selftext,\n",
    "                    'score': post.score,\n",
    "                    'created_utc': post_date,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'complaints': post_complaints\n",
    "                })\n",
    "                \n",
    "                for complaint in post_complaints:\n",
    "                    complaints.append({\n",
    "                        'text': complaint,\n",
    "                        'score': post.score\n",
    "                    })\n",
    "                \n",
    "                print(f\"Found relevant post {len(posts)} (from {post_date.strftime('%Y-%m-%d')})\")\n",
    "    \n",
    "    return posts, complaints\n",
    "\n",
    "def analyze_complaints(complaints):\n",
    "    # Convert complaints to DataFrame\n",
    "    df_complaints = pd.DataFrame(complaints)\n",
    "    \n",
    "    # Get all complaint texts\n",
    "    complaint_texts = df_complaints['text'].tolist()\n",
    "    \n",
    "    # Analyze topics\n",
    "    print(\"\\nAnalyzing topics...\")\n",
    "    topics = analyze_topics(complaint_texts)\n",
    "    \n",
    "    # Get top bigrams\n",
    "    print(\"\\nAnalyzing bigrams...\")\n",
    "    top_bigrams = get_top_ngrams(complaint_texts, n=2)\n",
    "    \n",
    "    return topics, top_bigrams\n",
    "\n",
    "def main():\n",
    "    # Install required packages if not already installed\n",
    "    try:\n",
    "        import praw\n",
    "        import pandas as pd\n",
    "        from textblob import TextBlob\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.decomposition import NMF\n",
    "        import nltk\n",
    "    except ImportError:\n",
    "        print(\"Installing required packages...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call(['pip', 'install', 'praw', 'pandas', 'textblob', 'matplotlib', 'seaborn', 'scikit-learn', 'nltk'])\n",
    "        print(\"Packages installed successfully!\")\n",
    "\n",
    "    # Scrape posts\n",
    "    print(\"Starting to scrape Reddit posts...\")\n",
    "    posts, complaints = scrape_reddit_posts()\n",
    "    \n",
    "    # Convert posts to DataFrame\n",
    "    df_posts = pd.DataFrame(posts)\n",
    "    print(f\"\\nTotal relevant posts found: {len(df_posts)}\")\n",
    "    \n",
    "    if len(df_posts) == 0:\n",
    "        print(\"No relevant posts were found. Please check the scraping process.\")\n",
    "        return\n",
    "    \n",
    "    # Analyze complaints\n",
    "    topics, top_bigrams = analyze_complaints(complaints)\n",
    "    \n",
    "    # Create summary DataFrame for topics\n",
    "    topic_data = []\n",
    "    for i, topic in enumerate(topics, 1):\n",
    "        topic_data.append({\n",
    "            'Topic': f'Topic {i}',\n",
    "            'Top Words': ', '.join(topic)\n",
    "        })\n",
    "    \n",
    "    topic_df = pd.DataFrame(topic_data)\n",
    "    \n",
    "    # Create summary DataFrame for bigrams\n",
    "    bigram_data = []\n",
    "    for bigram, count in top_bigrams:\n",
    "        bigram_data.append({\n",
    "            'Bigram': ' '.join(bigram),\n",
    "            'Count': count\n",
    "        })\n",
    "    \n",
    "    bigram_df = pd.DataFrame(bigram_data)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop Topics:\")\n",
    "    print(\"===========\")\n",
    "    print(topic_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nTop Bigrams:\")\n",
    "    print(\"============\")\n",
    "    print(bigram_df.to_string(index=False))\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=bigram_df, x='Bigram', y='Count')\n",
    "    plt.title('Top Bigrams in Job Hunting Complaints')\n",
    "    plt.xlabel('Bigram')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('complaint_bigrams.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    df_posts.to_csv('job_hunting_posts.csv', index=False)\n",
    "    topic_df.to_csv('topic_analysis.csv', index=False)\n",
    "    bigram_df.to_csv('bigram_analysis.csv', index=False)\n",
    "    \n",
    "    # Save detailed summary\n",
    "    with open('complaint_analysis.txt', 'w') as f:\n",
    "        f.write(\"Job Hunting Complaints Analysis (Last Year)\\n\")\n",
    "        f.write(\"========================================\\n\\n\")\n",
    "        f.write(\"Top Topics:\\n\")\n",
    "        f.write(\"----------\\n\\n\")\n",
    "        for _, row in topic_df.iterrows():\n",
    "            f.write(f\"{row['Topic']}:\\n\")\n",
    "            f.write(f\"- Top Words: {row['Top Words']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"\\nTop Bigrams:\\n\")\n",
    "        f.write(\"------------\\n\\n\")\n",
    "        for _, row in bigram_df.iterrows():\n",
    "            f.write(f\"- {row['Bigram']}: {row['Count']} occurrences\\n\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to:\")\n",
    "    print(\"- job_hunting_posts.csv\")\n",
    "    print(\"- topic_analysis.csv\")\n",
    "    print(\"- bigram_analysis.csv\")\n",
    "    print(\"- complaint_analysis.txt\")\n",
    "    print(\"- complaint_bigrams.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'events'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.eventbriteapi.com/v3/events/search/\u001b[39m\u001b[38;5;124m'\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'events'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "token = 'EMMSKJGCMIW7BSU47IX7'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {token}'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'q': 'tech',\n",
    "    'location.address': 'Toronto',\n",
    "    'sort_by': 'date',\n",
    "    'start_date.range_start': '2025-04-01T00:00:00Z',\n",
    "    'start_date.range_end': '2025-04-30T23:59:59Z'\n",
    "}\n",
    "\n",
    "response = requests.get('https://www.eventbriteapi.com/v3/events/search/', headers=headers, params=params)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "for event in data['events']:\n",
    "    print(f\"Name: {event['name']['text']}\")\n",
    "    print(f\"Start: {event['start']['local']}\")\n",
    "    print(f\"URL: {event['url']}\")\n",
    "    print('-' * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
